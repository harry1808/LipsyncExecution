# ğŸ“Š Video Dubbing System - Evaluation Guide

## What You Can Evaluate and Show

This evaluation framework helps you measure and demonstrate the quality of your video dubbing system across multiple dimensions.

---

## ğŸ¯ What to Evaluate

### 1. **Speech Recognition Accuracy (ASR)**
**What it measures:** How accurately the system transcribes the original audio

**Metrics you'll get:**
- **WER (Word Error Rate)**: Lower is better (0% = perfect)
  - `WER = (Substitutions + Deletions + Insertions) / Total Words Ã— 100%`
  - **Good**: < 20%, **Excellent**: < 10%
  
- **CER (Character Error Rate)**: Character-level accuracy
  - Useful for languages without clear word boundaries
  
- **Accuracy**: Percentage of correctly recognized words

**Example Result:**
```
WER: 12.5%
Accuracy: 87.5%
Ground Truth: "Hello everyone welcome to this tutorial"
Recognized:   "Hello everyone welcome to the tutorial"
```

---

### 2. **Translation Quality**
**What it measures:** How accurately the system translates from source to target language

**Metrics you'll get:**
- **BLEU Score**: Translation quality (0-1 scale, higher is better)
  - `BLEU = BP Ã— exp(Î£ wâ‚™ Ã— log(pâ‚™))`
  - **Acceptable**: > 0.5, **Good**: > 0.7
  - Shows n-gram precision:
    - BLEU-1: Individual word accuracy
    - BLEU-2: Word pair accuracy
    - BLEU-3: Phrase accuracy (3 words)
    - BLEU-4: Phrase accuracy (4 words)

**Example Result:**
```
BLEU Score: 0.7234
BLEU-1: 0.85  (85% of words are correct)
BLEU-2: 0.76  (76% of word pairs match)
BLEU-3: 0.68
BLEU-4: 0.62
```

---

### 3. **Audio Duration Accuracy**
**What it measures:** How well the dubbed audio timing matches the original

**Metrics you'll get:**
- **Duration Error (%)**: Percentage difference in length
- **Duration Ratio**: Synthesized length / Original length
- **Absolute Difference**: Time difference in seconds

**Example Result:**
```
Original Duration: 15.5s
Dubbed Duration: 16.2s
Difference: 0.7s
Error: 4.5%
```

---

### 4. **Overall Quality Score**
**What it measures:** Composite quality score combining all metrics

**How it's calculated:**
```
Composite Score = 0.25Ã—(100-WER) + 0.30Ã—(BLEUÃ—100) + 0.25Ã—(100-CER) + 0.20Ã—(100-DurError)
```

**Rating Scale:**
- 90-100: â­â­â­â­â­ **EXCELLENT** - Production ready
- 75-89:  â­â­â­â­ **GOOD** - Minor improvements needed
- 60-74:  â­â­â­ **FAIR** - Acceptable for some use cases
- 40-59:  â­â­ **POOR** - Needs significant work
- 0-39:   â­ **NEEDS IMPROVEMENT** - Major issues

---

## ğŸš€ How to Run Evaluation

### **Method 1: Quick Single Video Test**

```bash
python run_evaluation.py --quick \
  --video test_video.mp4 \
  --source-lang en \
  --dest-lang hi \
  --transcript "Hello everyone, welcome to this tutorial" \
  --translation "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¸à¤­à¥€ à¤•à¥‹, à¤‡à¤¸ à¤Ÿà¥à¤¯à¥‚à¤Ÿà¥‹à¤°à¤¿à¤¯à¤² à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ" \
  --html
```

### **Method 2: Batch Evaluation (Multiple Videos)**

1. **Create test configuration file** (see `test_data_template.json`):
```json
{
  "test_cases": [
    {
      "id": "test_001",
      "video_path": "path/to/video1.mp4",
      "source_lang": "en",
      "dest_lang": "hi",
      "ground_truth": {
        "transcript": "Original text",
        "translation": "Expected translation"
      }
    }
  ]
}
```

2. **Run batch evaluation**:
```bash
python run_evaluation.py --config test_data_template.json --html
```

### **Method 3: Python API**

```python
from webapp.evaluate_dubbing import DubbingEvaluator

evaluator = DubbingEvaluator()

results = evaluator.evaluate_full_pipeline(
    video_path="test_video.mp4",
    source_lang="en",
    dest_lang="hi",
    ground_truth={
        'transcript': "Hello everyone",
        'translation': "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¸à¤­à¥€ à¤•à¥‹"
    }
)

print(evaluator.generate_report(results))
```

---

## ğŸ“ˆ What Results You Get

### **1. Console Output (Real-time)**
```
==================================================================
VIDEO DUBBING SYSTEM - EVALUATION REPORT
==================================================================

Video: test_video.mp4
Languages: en â†’ hi
Status: SUCCESS

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMPONENT RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[1] Speech Recognition (ASR):
    Word Error Rate (WER):  12.50%
    Character Error Rate:    8.30%
    Accuracy:               87.50%

[2] Translation:
    BLEU Score:              0.7234
    BLEU-1 (unigrams):       0.8500
    BLEU-2 (bigrams):        0.7600

[3] Duration Accuracy:
    Original Duration:       15.50s
    Dubbed Duration:         16.20s
    Error Percentage:        4.50%

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OVERALL QUALITY SCORE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Score: 82.30/100
    Rating: â­â­â­â­ GOOD
```

### **2. JSON File (Machine Readable)**
Location: `evaluation_output/evaluation_results.json`

```json
{
  "video_path": "test_video.mp4",
  "source_lang": "en",
  "dest_lang": "hi",
  "status": "success",
  "components": {
    "asr": {
      "wer": 12.5,
      "cer": 8.3,
      "accuracy": 87.5
    },
    "translation": {
      "bleu_score": 0.7234,
      "bleu_1": 0.85
    }
  },
  "composite_score": 82.3
}
```

### **3. HTML Report (Beautiful Visual)**
Location: `evaluation_output/report.html`

Features:
- âœ… Colorful progress bars
- âœ… Side-by-side text comparison
- âœ… Metric breakdowns
- âœ… Score badges
- âœ… Professional formatting

![Sample HTML Report](docs/sample_report_preview.png)

### **4. Batch Summary (Multiple Videos)**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               BATCH EVALUATION SUMMARY                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total Test Cases: 5                                            â•‘
â•‘ Successful: 5                                                  â•‘
â•‘ Success Rate: 100.0%                                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Metric        â”‚ Mean   â”‚ Std Dev â”‚ Min    â”‚ Max               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ WER (%)       â”‚ 15.20  â”‚ 2.60    â”‚ 12.50  â”‚ 18.90            â•‘
â•‘ BLEU          â”‚ 0.6880 â”‚ 0.0300  â”‚ 0.6520 â”‚ 0.7230           â•‘
â•‘ Quality Score â”‚ 78.70  â”‚ 3.30    â”‚ 74.50  â”‚ 82.30            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“Š How to Present Results

### **For Academic Papers / Reports**

**Table Format:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Language Pair  â”‚ WER (%) â”‚  BLEU   â”‚ Duration    â”‚ Composite   â”‚
â”‚                â”‚         â”‚         â”‚ Error (%)   â”‚ Score       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ EN â†’ HI        â”‚  12.5   â”‚  0.723  â”‚     4.5     â”‚    82.3     â”‚
â”‚ EN â†’ TA        â”‚  15.2   â”‚  0.681  â”‚     6.2     â”‚    78.1     â”‚
â”‚ EN â†’ BN        â”‚  18.9   â”‚  0.652  â”‚     5.8     â”‚    74.5     â”‚
â”‚ HI â†’ EN        â”‚  14.3   â”‚  0.695  â”‚     3.9     â”‚    79.8     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Average        â”‚ 15.2Â±2.6â”‚ 0.688Â±  â”‚   5.1Â±1.0   â”‚  78.7Â±3.3   â”‚
â”‚                â”‚         â”‚  0.030  â”‚             â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Text to Include in Paper:**
```
Our system achieved an average Word Error Rate of 15.2% (Â±2.6%) 
across 4 language pairs, indicating high accuracy in transcription. 
Translation quality measured by BLEU score averaged 0.688 (Â±0.030), 
demonstrating strong semantic preservation. The composite quality 
score of 78.7/100 indicates production-ready performance.
```

### **For Presentations**

**Key Highlights:**
- ğŸ¯ **87.5% Transcription Accuracy** (WER: 12.5%)
- ğŸŒ **Quality Translation** (BLEU: 0.72)
- â±ï¸ **Excellent Timing** (4.5% duration error)
- â­ **Overall Score: 82.3/100** (GOOD rating)

### **For Documentation**

Use the HTML report - it's:
- Professional looking
- Easy to understand
- Includes all details
- Shareable via browser

---

## ğŸ§ª Best Practices

### **Test Data Preparation**
1. **Collect diverse videos**:
   - Different speakers (male/female)
   - Various audio quality levels
   - Multiple domains (education, news, casual)
   - Different lengths (5s to 60s)

2. **Create ground truth**:
   - Manually verify transcripts (critical!)
   - Use professional translators for translations
   - Or use verified MT with human review

3. **Minimum test set**:
   - 10-20 videos per language pair
   - Balance across different conditions

### **Running Evaluations**
1. Start with quick single tests
2. Debug any issues
3. Run full batch evaluation
4. Analyze aggregate metrics

### **Reporting Results**
1. Always show mean Â± std deviation
2. Include min/max ranges
3. Compare with baselines if available
4. Highlight both strengths and limitations

---

## ğŸ“ File Structure

```
lipsyncExecution/
â”œâ”€â”€ webapp/
â”‚   â”œâ”€â”€ evaluation_metrics.py      # Core metric calculations
â”‚   â”œâ”€â”€ evaluate_dubbing.py        # Evaluation pipeline
â”‚   â””â”€â”€ evaluation_visualizer.py   # Report generation
â”œâ”€â”€ example_evaluation.py          # Usage examples
â”œâ”€â”€ run_evaluation.py              # CLI runner
â”œâ”€â”€ test_data_template.json        # Test data template
â”œâ”€â”€ EVALUATION_README.md           # This file
â””â”€â”€ evaluation_output/             # Results (auto-created)
    â”œâ”€â”€ evaluation_results.json    # Detailed results
    â”œâ”€â”€ report.html                # Visual report
    â””â”€â”€ batch_results.json         # Batch summary
```

---

## ğŸ” Interpreting Metrics

### **WER (Word Error Rate)**
- **< 10%**: Excellent - Near-human accuracy
- **10-20%**: Good - Acceptable for most use cases
- **20-30%**: Fair - May need manual correction
- **> 30%**: Poor - System needs improvement

### **BLEU Score**
- **> 0.7**: Good - Captures meaning well
- **0.5-0.7**: Acceptable - Some semantic loss
- **0.3-0.5**: Fair - Significant differences
- **< 0.3**: Poor - Major translation issues

### **Duration Error**
- **< 5%**: Excellent - Natural timing
- **5-10%**: Good - Barely noticeable
- **10-20%**: Fair - May need video adjustment
- **> 20%**: Poor - Significant timing issues

---

## ğŸ› ï¸ Troubleshooting

**Q: WER is very high (>30%)**
- Check audio quality
- Verify correct language code
- Test Whisper model directly
- Consider fine-tuning ASR model

**Q: BLEU score is low (<0.3)**
- Verify ground truth translation
- Check if domain-specific terminology
- Test NLLB model directly
- Consider using different translation model

**Q: Duration error is large (>20%)**
- Source and target languages may have different speech rates
- Check if TTS synthesis is too slow/fast
- May need to adjust TTS parameters

**Q: System crashes during evaluation**
- Check GPU memory availability
- Reduce batch size
- Verify all dependencies installed
- Check video file integrity

---

## ğŸ“š References

**Metrics:**
- BLEU: [Papineni et al., 2002](https://aclanthology.org/P02-1040/)
- WER: Standard speech recognition metric
- MCD: [Kubichek, 1993]

**Models:**
- Whisper ASR: [OpenAI Whisper](https://github.com/openai/whisper)
- NLLB Translation: [Meta NLLB](https://github.com/facebookresearch/fairseq/tree/nllb)
- Indic TTS: [AI4Bharat](https://github.com/AI4Bharat/Indic-TTS)

---

## ğŸ’¡ Quick Start

```bash
# 1. Run example to see what results look like
python example_evaluation.py

# 2. Create your test data file
cp test_data_template.json my_tests.json
# Edit my_tests.json with your video paths

# 3. Run evaluation
python run_evaluation.py --config my_tests.json --html

# 4. View results
# Open: evaluation_output/report.html in browser
```

---

## ğŸ“ Support

For questions or issues:
1. Check `example_evaluation.py` for usage examples
2. Review this README
3. Check evaluation logs: `evaluation.log`

---

**Happy Evaluating! ğŸ‰**

Show your results with confidence - the metrics are industry-standard and the reports are professional! âœ¨

